Bias and Toxicity in LLMs

Although there may be a thread of positive bias, for the most part bias is a negative trait baked into LLMs.  As Bender et. al. pointed out, many models are biased and as a consequence promote toxicity in a variety of forms from blatant forms to more subtler forms of harm. Bias starts during the encoding process, whether the LLM gathers its data from Common Crawl, which has a disproportionate amount of fringe and discriminatory sites, or to encoding bias, reflecting the biases of the encoders.  There may be auditing protocols to lessen the encoding biases in models, but one asks what about the biases of the auditors and how they are considered. Bias comes in a range of forms. There is gender bias, which can be seen when one is forced to state ‘woman doctor’ instead of ‘doctor’ when they are inputting a request.  There is professions’ bias when the LLM skews completions towards high status professions, such as executives rather than taxi drivers. There is also more subtler forms such as socio-political framing of completions. Bias has consequences to a culture and has ripple effects of harm ranging from hurtful sentence completions towards sexist, racist ideologies, or to harmful stereotypes towards certain groups of society.   These ripple effects not only perpetuate hateful attitudes but magnify them even leading to physical harm. Again, there are subtler forms of harm that an LLM can perpetuate.  Even though an individual’s personal identifiable information (PPI) is scattered online, LLMs gather it all up and make it easily available further eroding a person’s right to privacy.   As Ruha Benjamin noted, “Feeding AI systems on the world’s beauty, ugliness and cruelty but expecting it to reflect only the beauty is fantasy.” It is the responsibility of a conscientious individual to not only be aware of these issues but to also work in their won capacity to eradicate them. 
